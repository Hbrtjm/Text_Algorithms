{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6fd7a71-74cc-45fa-94e6-0aa855b6d975",
   "metadata": {},
   "source": [
    "# Sprawozdanie z laboratorium 2 z algorytmów tekstowych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d601b-804f-4b6a-b66a-a625e091715c",
   "metadata": {},
   "source": [
    "**Hubert Miklas 03-04-2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d00ff7-ceff-40cc-a842-3d0770871bdf",
   "metadata": {},
   "source": [
    "## Wstęp\n",
    "\n",
    "Tematem laboratorium były wyrażenia regularne. Do każdego zadania przedstawiony jest szczegółowy opis rozwiązania."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528105ac-abcb-437b-bb63-905bf8bbebd0",
   "metadata": {},
   "source": [
    "## Zadanie 1 - Parsowanie cytowań artykułów naukowych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b5ac7-effa-4690-bc85-c15bf423c93b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Poniżej opisany kod służy do ekstrakcji informacji z cytowania publikacji naukowych w zadanym formacie. Korzstająz z RegEx należało wydobyć określone dane - imiona i nazwiska autorów, rok publikacji, tytuł, nazwa czasopisma, tom, numer oraz zakres stron.\n",
    "\n",
    "### Opis działania kodu\n",
    "\n",
    "#### 1. Definicja wyrażeń regularnych\n",
    "\n",
    "##### a) Wzorzec dla autorów i roku publikacji\n",
    "```python\n",
    "authors_year_pattern = r\"(?P<authors>(([A-ZŻŹĆĄŚĘŁÓŃ][a-zżźćńółąśę]+\\s*,\\s*[A-ZŻŹĆĄŚĘŁÓŃ]\\.\\s*),{0,1}\\s*)+)\\((?P<year>\\d{4})\\)\"\n",
    "```\n",
    "**Opis:**\n",
    "- `(?P<authors>(...)+)`: Grupa nazwanych autorów, zawierająca jeden lub więcej autorów.\n",
    "- `([A-ZŻŹĆĄŚĘŁÓŃ][a-zżźćńółąśę]+\\s*,\\s*[A-ZŻŹĆĄŚĘŁÓŃ]\\.)`: Dopasowanie nazwiska i inicjału autora.\n",
    "- `,{0,1}\\s*`: Opcjonalna dodatkowa przecinek i odstęp dla kolejnych autorów.\n",
    "- `\\((?P<year>\\d{4})\\)`: Rok w nawiasach jako liczba czterocyfrowa.\n",
    "\n",
    "##### b) Wzorzec dla tytułu i nazwy czasopisma\n",
    "```python\n",
    "title_journal_pattern = r\"\\.\\s*(?P<title>.+?)\\.\\s*(?P<journal>.+?),\"\n",
    "```\n",
    "**Opis:**\n",
    "- `\\.\\s*`: Dopasowanie kropki i ewentualnych odstępów. Kropka w tym kotekście oznacza koniec listy z autorami.\n",
    "- `(?P<title>.+?)`: Dopasowanie tytułu, ze względu na dowolne znaki dopasowanie jest przez wildcard (.).\n",
    "- `\\.\\s*`: Kolejna kropka i odstępy.\n",
    "- `(?P<journal>.+?),`: Dopasowanie nazwy czasopisma do przecinka.\n",
    "\n",
    "##### c) Wzorzec dla numeru tomu, numeru czasopisma i stron\n",
    "```python\n",
    "volume_issue_pages_pattern = r\"\\s*(?P<volume>\\d+)(?:\\((?P<issue>\\d+)\\))?,\\s*(?P<start_page>\\d+)-(?P<end_page>\\d+)\\.\"\n",
    "```\n",
    "**Opis:**\n",
    "- `(?P<volume>\\d+)`: Dopasowanie numeru tomu.\n",
    "- `(?:\\((?P<issue>\\d+)\\))?`: Opcjonalne dopasowanie numeru wydania.\n",
    "- `(?P<start_page>\\d+)-(?P<end_page>\\d+)`: Dopasowanie zakresu stron.\n",
    "- `.\\`: Kolejna sekcja zakończona kropką.\n",
    "\n",
    "#### 2. Łączenie wzorców w jedno wyrażenie\n",
    "```python\n",
    "full_pattern = authors_year_pattern + title_journal_pattern + volume_issue_pages_pattern\n",
    "```\n",
    "Złożenie wszystkich wzorców w jeden kompletny wzorzec dla całej referencji.\n",
    "\n",
    "#### 3. Dopasowanie wzorca do podanego tekstu\n",
    "```python\n",
    "matches = re.match(full_pattern, reference)\n",
    "if not matches:\n",
    "    return None\n",
    "```\n",
    "Jeśli `reference` nie pasuje do wzorca, funkcja zwraca `None`.\n",
    "\n",
    "#### 4. Wydobycie listy autorów\n",
    "```python\n",
    "authors_text = matches.group(\"authors\")\n",
    "author_pattern = r\"(?P<last_name>[A-ZŻŹĆĄŚĘŁÓŃ][a-zżźćńółąśę]+),\\s*(?P<initial>[A-ZŻŹĆĄŚĘŁÓŃ])\\.,{0,1}\\s\"\n",
    "authors_list = []\n",
    "for author in re.finditer(author_pattern, authors_text):        \n",
    "    authors_list.append({\"last_name\": author.group('last_name'), \"initial\": author.group('initial')})\n",
    "```\n",
    "**Opis:**\n",
    "- `authors_text` zawiera całą część referencji dotyczącą autorów.\n",
    "- `author_pattern` dopasowuje pojedynczego autora.\n",
    "- `re.finditer()` iteruje przez wszystkich autorów i tworzy listę słowników z nazwiskiem i inicjałem.\n",
    "\n",
    "#### 5. Tworzenie wyniku w formie słownika\n",
    "```python\n",
    "result = {\n",
    "    \"authors\": authors_list,\n",
    "    \"year\": int(matches.group(\"year\")),\n",
    "    \"title\": matches.group(\"title\"),\n",
    "    \"journal\": matches.group(\"journal\"),\n",
    "    \"volume\": int(matches.group(\"volume\")),\n",
    "    \"issue\": int(matches.group(\"issue\")) if matches.group(\"issue\") else None,\n",
    "    \"pages\": {\n",
    "        \"start\": int(matches.group(\"start_page\")),\n",
    "        \"end\": int(matches.group(\"end_page\")),\n",
    "    },\n",
    "}\n",
    "```\n",
    "Dane są wyodrębniane z dopasowań i konwertowane na odpowiednie typy:\n",
    "- Lista słowników dla autorów.\n",
    "- Liczba całkowita dla roku, tomu i numeru (jeśli istnieje).\n",
    "- Słownik dla stron zawierający wartości liczbowe.\n",
    "\n",
    "Funkcja zwraca kompletny słownik z wyodrębnionymi informacjami.\n",
    "\n",
    "Kod został przetestowany i przeszedł wszystkie testy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9695f3f5-4c6a-4c3b-8ab0-c9b448804b53",
   "metadata": {},
   "source": [
    "## Zadanie 2 -  Ekstrakcja linków z HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b752268b-2732-482b-8ffa-a8df2ea0ba1f",
   "metadata": {},
   "source": [
    "Poniżej opisany kod służy do analizy fragmentu kodu HTML i wyodrębniania wszystkich linków (`<a>` tagów). Wykorzystuje wyrażenia regularne (`RegEx`) do dopasowania oraz ekstrakcji kluczowych informacji, takich jak:\n",
    "- `href` – adres URL,\n",
    "- `title` – wartość atrybutu `title` (jeśli istnieje),\n",
    "- `text` – tekst wyświetlany jako link.\n",
    "\n",
    "### Szczegółowy opis działania kodu\n",
    "\n",
    "#### 1. Definicja wyrażenia regularnego\n",
    "```python\n",
    "pattern = r\"<a\\s+href=\\\"(?P<url>http(s){0,1}:\\/\\/[^\\\"]+)\\\"(\\s+title\\s*=\\s*\\\"(?P<title>(.+))\\\"){0,1}>(?P<text>[A-ZŻŹĆĄŚĘŁÓŃa-zżźćńółąśę\\s,.]+)<\\/a>\"\n",
    "```\n",
    "**Opis:**\n",
    "- `<a\\s+` – Dopasowuje tag `<a>` z ewentualnymi spacjami.\n",
    "- `href=\\\"(?P<url>http(s){0,1}:\\/\\/[^\\\"]+)\\\"` – Grupuje wartość `href` jako `url`:\n",
    "  - `http(s){0,1}:\\/\\/` – Obsługuje `http://` oraz `https://`.\n",
    "  - `[^\\\"]+` – Dopasowuje dowolny ciąg znaków poza `\"`.\n",
    "- `(\\s+title\\s*=\\s*\\\"(?P<title>(.+))\\\"){0,1}` – Opcjonalne dopasowanie atrybutu `title`:\n",
    "  - `\\s+title\\s*=\\s*\\\"(?P<title>(.+))\\\"` – Pobiera wartość `title`.\n",
    "  - `{0,1}` – Wskazuje, że `title` może, ale nie musi występować.\n",
    "- `>(?P<text>[A-ZŻŹĆĄŚĘŁÓŃa-zżźćńółąśę\\s,.]+)<\\/a>` – Dopasowuje tekst linku.\n",
    "  \n",
    "#### 2. Iteracja po dopasowaniach i ekstrakcja danych\n",
    "```python\n",
    "searched = [\"title\",\"url\",\"text\"]\n",
    "links = []\n",
    "for element in re.finditer(pattern,html):\n",
    "    dic = {}\n",
    "    for name in searched:\n",
    "        dic[name] = element.groupdict().get(name)\n",
    "    links.append(dic)\n",
    "```\n",
    "**Proces:**\n",
    "1. `re.finditer(pattern, html)` – Znajduje wszystkie dopasowania w tekście.\n",
    "2. Dla każdego dopasowania:\n",
    "   - Tworzy pusty słownik `dic`.\n",
    "   - Iteruje przez listę `searched` (`title`, `url`, `text`).\n",
    "   - Pobiera wartości z `groupdict()` i dodaje je do słownika `dic`.\n",
    "   - Dodaje słownik `dic` do listy `links`.\n",
    "\n",
    "Funkcja zwraca listę słowników, gdzie każdy słownik zawiera kluczowe informacje o znalezionych linkach.\n",
    "\n",
    "Kod został przetestowany i przeszedł wszystkie testy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3c465-baa1-417c-9f77-1c27573008af",
   "metadata": {},
   "source": [
    "## Zadanie 3 - Analiza publikacji w formacie piku tekstowego"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455ed69-0fac-4dee-a4e3-08f89789fb13",
   "metadata": {},
   "source": [
    "Poniżej opisany kod analizuje plik tekstowy pod kątem statystyk i wzorców, wykorzystując RegEx do ekstrakcji dat i adresów email, a także do zliczenia słów, zdań i paragrafów. Poniżej znajduje się szczegółowy opis procesu parsowania pliku oraz zastosowanych wyrażeń regularnych.\n",
    "\n",
    "### Opis działania kodu\n",
    "\n",
    "#### 1. Wyodrębnienie słów\n",
    "Zastosowane wyrażenie regularne wyszukuje wszystkie słowa składające się z małych i wielkich liter. Przetworzone słowa są konwertowane na małe litery dla ułatwienia dalszej analizy, w szczególności przy usuwaniu 'stop words'.\n",
    "\n",
    "```python\n",
    "word_regex = r'(?P<word>([A-Z]|[a-z])[a-z]*)(\\s|\\,|\\.|\\!|\\?)'\n",
    "words = [word.group(\"word\").lower() for word in re.finditer(word_regex, content)]\n",
    "word_count = len(words)\n",
    "```\n",
    "\n",
    "**Opis**\n",
    "1. `(?P<word>...)` – Grupę `word`, umożliwiając późniejszy dostęp do dopasowanego słowa.  \n",
    "2. `([A-Z]|[a-z])` – Słowo musi zaczynać się od litery (wielkiej lub małej).  \n",
    "3. `[a-z]*` – Po pierwszej literze mogą wystąpić dowolne małe litery (zero lub więcej razy).  \n",
    "4. `(\\s|\\,|\\.|\u0001|\\?)` – Słowo kończy się białym znakiem lub znakami interpunkcyjnymi: `, . ! ?`.  \n",
    "\n",
    "\n",
    "#### 2. Podział tekstu na zdania\n",
    "Funkcja wyszukuje zdania na podstawie znaków interpunkcyjnych takich jak `.`, `!`, `?`, a następnie liczy ich liczbę.\n",
    "\n",
    "```python\n",
    "sentence_pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "sentences = [element for element in re.finditer(sentence_pattern, content)]\n",
    "sentence_count = len([s for s in sentences if str(s).strip()])\n",
    "```\n",
    "\n",
    "**Opis**\n",
    "1. `(?<!\\w\\.\\w.)` – Negatywny lookbehind, ignorujący skróty takie jak `U.S.A.`.  \n",
    "2. `(?<![A-Z][a-z]\\.)` – Negatywny lookbehind ignorujący skróty jak `Dr.` czy `Mr.`.  \n",
    "3. `(?<=\\.|\\?|\\!)\\s` – Zdanie kończy się `.`, `?` lub `!`, po którym następuje spacja.  \n",
    "\n",
    "#### 3. Szukanie adresów e-mail\n",
    "Adresy e-mail są wykrywane za pomocą wyrażenia regularnego, które obsługuje zarówno standardowe domeny, jak i złożone nazwy użytkowników.\n",
    "\n",
    "```python\n",
    "email_pattern = r\"([A-Za-z\\-]+\\.)*[A-Za-z\\-]+@([A-Za-z\\-]+\\.)+[A-Za-z\\-]+\"\n",
    "emails = [email.group() for email in re.finditer(email_pattern, content)]\n",
    "```\n",
    "\n",
    "**Opis**\n",
    "1. `([A-Za-z\\-]+\\.)*` – Opcjonalna część identyfikatora użytkownika.  \n",
    "2. `[A-Za-z\\-]+@` – Nazwa użytkownika zawierająca litery i `-`, zakończona `@`.  \n",
    "3. `([A-Za-z\\-]+\\.)+[A-Za-z\\-]+` – Domena, np. `gmail.com`. \n",
    "\n",
    "#### 4. Liczenie częstości występowania słów\n",
    "Lista słów jest filtrowana tak, aby usunąć często występujące angielskie słowa (stop words), a następnie zliczana jest ich częstość występowania.\n",
    "\n",
    "```python\n",
    "stop_words = [...]  # Cały zbiór stop words\n",
    "new_words_order = filter(lambda word: word not in stop_words, words)\n",
    "frequent_words = {}\n",
    "for word in new_words_order:\n",
    "    if word not in frequent_words:\n",
    "        frequent_words[word] = 1\n",
    "    else:\n",
    "        frequent_words[word] += 1\n",
    "most_frequent_words = {key: value for key, value in list(filter(lambda x: x[1] > 1, sorted(frequent_words.items(), key=lambda x: x[1], reverse=True)))}\n",
    "```\n",
    "\n",
    "#### 5. Ekstrakcja dat w różnych formatach\n",
    "Funkcja wykrywa daty zapisane w różnych formatach, np. `YYYY-MM-DD`, `DD.MM.YYYY`, `MM/DD/YYYY`, itd.\n",
    "\n",
    "```python\n",
    "date_patterns = [\n",
    "    r\"(\\d{4})-(0[1-9]|1[0-2])-([0-2][0-9]|3[0-1])\",  # YYYY-MM-DD\n",
    "    r\"([0-2][0-9]|3[0-1])-(0[1-9]|1[0-2])-(\\d{4})\",  # DD-MM-YYYY\n",
    "    r\"(0[1-9]|1[0-2])-([0-2][0-9]|3[0-1])-(\\d{4})\",  # MM-DD-YYYY\n",
    "    r\"([0-2][0-9]|3[0-1])\\.(0[1-9]|1[0-2])\\.(\\d{4})\",  # DD.MM.YYYY\n",
    "    r\"(0[1-9]|1[0-2])\\/([0-2][0-9]|3[0-1])\\/(\\d{4})\",  # MM/DD/YYYY\n",
    "    r\"(\\d{4})\\/(0[1-9]|1[0-2])\\/([0-2][0-9]|3[0-1])\",  # YYYY/MM/DD\n",
    "    r\"(\\d{4})\\.(0[1-9]|1[0-2])\\.([0-2][0-9]|3[0-1])\",  # YYYY.MM.DD\n",
    "    r\"(January|February|March|April|May|June|July|August|September|October|November|December) ([0-2][0-9]|3[0-1]), (\\d{4})\",  # Month DD, YYYY\n",
    "    r\"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) ([0-2][0-9]|3[0-1]), (\\d{4})\"  # MMM DD, YYYY\n",
    "]\n",
    "dates = []\n",
    "for pattern in date_patterns:\n",
    "    dates += [date.group() for date in re.finditer(pattern, content)]\n",
    "```\n",
    "\n",
    "#### 6. Podział tekstu na akapity\n",
    "Akapity są wykrywane na podstawie pustych linii. Dla każdego akapitu liczona jest liczba słów.\n",
    "\n",
    "```python\n",
    "paragraphs = re.finditer(r'.+\\n+', content)\n",
    "paragraph_sizes = {i: count_words(word_regex, paragraph.group()) for i, paragraph in enumerate(paragraphs)}\n",
    "paragraph_sizes = {key: value for key, value in paragraph_sizes.items() if value != 0}\n",
    "```\n",
    "\n",
    "**Opis**\n",
    "- `.+\\n+` - Wyszukuje niepuste linie zakończone znakiem nowej linii (`\\n`).\n",
    "- `count_words()` - Oblicza liczbę słów w danym akapicie.\n",
    "\n",
    "#### 7. Zwrócenie wyników\n",
    "Funkcja zwraca słownik z wszystkimi zgromadzonymi statystykami.\n",
    "\n",
    "```python\n",
    "result = {\n",
    "    \"word_count\": word_count,\n",
    "    \"sentence_count\": sentence_count,\n",
    "    \"emails\": emails,\n",
    "    \"frequent_words\": most_frequent_words,\n",
    "    \"dates\": dates,\n",
    "    \"paragraph_sizes\": paragraph_sizes,\n",
    "}\n",
    "return result\n",
    "```\n",
    "\n",
    "Funkcja zwraca wymagany słownik, w którym znajduje się liczba słów, liczba zdań, wykryte adresy email, częste słowa z wykluczeniem 'stop words', wykryte datu i wielkości paragrafów.\n",
    "\n",
    "Kod został przetestowany i przeszedł wszystkie testy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a218b089-e29e-4e3a-96f7-cb278513e976",
   "metadata": {},
   "source": [
    "## Zadanie 4 - Implementacja uproszczonego parsera wyrażeń regularnych do DFA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529d54e-6fd1-47a2-b7f5-d48a50ab7cb2",
   "metadata": {},
   "source": [
    "Celem tego zadania jest zaimplementowanie algorytmu Brzozowskiego, który konwertuje wyrażenia regularne na deterministyczny automat skończony (DFA). Należało zaimplementować lub przeanalizować zadane fragmenty: \n",
    "\n",
    "1. **Implementacje metody `nullable()`** dla różnych typów wyrażeń regularnych – określenie, czy dane wyrażenie akceptuje pusty ciąg.\n",
    "2. **Obliczanie pochodnej wyrażenia względem symbolu (`derivative()`)**, co pozwala określić, jak wyrażenie reaguje na przetworzenie pierwszego symbolu.\n",
    "3. **Uproszczenie (`simplify()`)** struktury wyrażenia w celu eliminacji zbędnych operacji i poprawy wydajności.\n",
    "4. **Budowę automatu DFA (`build_dfa()`)** poprzez iteracyjne stosowanie pochodnych Brzozowskiego i przechowywanie stanów w postaci wyrażeń regularnych.\n",
    "\n",
    "#### Struktura kodu\n",
    "\n",
    "Kod składa się z kilku klas reprezentujących wyrażenia regularne oraz funkcji obsługujących ich właściwości:\n",
    "\n",
    "- `RegEx` – klasa abstrakcyjna dla wszystkich wyrażeń regularnych.\n",
    "- `Empty` – reprezentacja pustego języka.\n",
    "- `Epsilon` – reprezentacja pustego ciągu (elementu neutralnego względem konkatenacji).\n",
    "- `Symbol` – reprezentacja pojedynczego symbolu.\n",
    "- `Concatenation` – reprezentacja konkatenacji dwóch wyrażeń.\n",
    "- `Alternative` – reprezentacja alternatywy dwóch wyrażeń.\n",
    "- `KleeneStar` – reprezentacja gwiazdki Kleene’a.\n",
    "- `DFA` – klasa reprezentująca automat deterministyczny.\n",
    "- `simplify()` – funkcja upraszczająca wyrażenia.\n",
    "- `build_dfa()` – funkcja implementująca algorytm Brzozowskiego do budowy DFA.\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementacja metod `nullable()`\n",
    "\n",
    "Metoda `nullable()` określa, czy dane wyrażenie regularne akceptuje pusty ciąg (`ε`).\n",
    "\n",
    "- **Empty**: `∅` nie akceptuje pustego ciągu -> `False`.\n",
    "- **Epsilon**: `ε` akceptuje pusty ciąg -> `True`.\n",
    "- **Symbol**: pojedynczy symbol nigdy nie akceptuje pustego ciągu -> `False`.\n",
    "- **Concatenation**: konkatenacja `r • s` akceptuje pusty ciąg, jeśli oba składniki są nullable -> `r.nullable() and s.nullable()`.\n",
    "- **Alternative**: alternatywa `r | s` akceptuje pusty ciąg, jeśli **co najmniej jedno** z wyrażeń jest nullable -> `r.nullable() or s.nullable()`.\n",
    "- **KleeneStar**: `r*` zawsze akceptuje pusty ciąg -> `True`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementacja metody `derivative()`\n",
    "\n",
    "Metoda `derivative(symbol)` oblicza pochodną Brzozowskiego wyrażenia względem podanego symbolu. Idea polega na tym, by sprawdzić, jak wyrażenie zmienia się po przetworzeniu pierwszego symbolu.\n",
    "\n",
    "- **Empty**: Pochodna pustego języka to nadal pusty język -> `∅`.\n",
    "- **Epsilon**: `ε` nie ma żadnej treści do przetworzenia, więc pochodna to `∅`.\n",
    "- **Symbol**: `D(a, a) = ε`, `D(a, b) = ∅`.\n",
    "- **Concatenation**: Pochodna `rs` względem `a` to:\n",
    "  ```\n",
    "  D(rs, a) = D(r, a) • s + δ(r) • D(s, a)\n",
    "  ```\n",
    "  gdzie `δ(r) = ε` jeśli `r.nullable()`, w przeciwnym razie `∅`.\n",
    "- **Alternative**: `D(r|s, a) = D(r, a) | D(s, a)`.\n",
    "- **KleeneStar**: `D(r*, a) = D(r, a) • r*`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Uproszczanie wyrażeń (`simplify()`)\n",
    "\n",
    "Funkcja `simplify()` upraszcza wyrażenia według reguł:\n",
    "- `r | ∅ = r`, `∅ | r = r`\n",
    "- `r ∅ = ∅`, `∅ r = ∅`\n",
    "- `r ε = r`, `ε r = r`\n",
    "- `(r*)* = r*`, `ε* = ε`, `∅* = ε`\n",
    "\n",
    "---\n",
    "\n",
    "#### Budowa automatu DFA (`build_dfa()`)\n",
    "\n",
    "Funkcja `build_dfa()` tworzy automat DFA na podstawie wyrażenia regularnego poprzez:\n",
    "1. Rozpoczęcie od początkowego wyrażenia jako pierwszego stanu.\n",
    "2. Iteracyjne obliczanie pochodnych dla każdego symbolu alfabetu.\n",
    "3. Przypisanie unikalnych stanów do wynikowych wyrażeń.\n",
    "4. Kontynuowanie, aż wszystkie stany zostaną przetworzone.\n",
    "5. Ustalanie stanów akceptujących jako tych, które są `nullable()`.\n",
    "\n",
    "Przykład użycia:\n",
    "```python\n",
    "# Wyrażenie regularne: (a|b)*abb\n",
    "regex = Concatenation(\n",
    "    Concatenation(\n",
    "        Concatenation(\n",
    "            KleeneStar(Alternative(Symbol('a'), Symbol('b'))),\n",
    "            Symbol('a')\n",
    "        ),\n",
    "        Symbol('b')\n",
    "    ),\n",
    "    Symbol('b')\n",
    ")\n",
    " \n",
    "# Sprawdzenie, czy łańcuch pasuje do wyrażenia\n",
    "dfa = build_dfa(regex, {'a', 'b'})\n",
    "assert dfa.accepts(\"abb\") == True\n",
    "assert dfa.accepts(\"aabb\") == True\n",
    "assert dfa.accepts(\"babb\") == True\n",
    "assert dfa.accepts(\"ab\") == False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Kroki algorytmu:\n",
    "1. **Inicjalizacja:** Rozpoczynamy od zainicjalizowania pierwszego stanu jako reprezentacji początkowego wyrażenia regularnego.\n",
    "2. **Przetwarzanie stanów:**\n",
    "   - Dla każdego stanu oraz każdego symbolu alfabetu:\n",
    "     1. Obliczamy pochodną wyrażenia regularnego względem danego symbolu.\n",
    "     2. Upraszczamy otrzymane wyrażenie regularne.\n",
    "     3. Dodajemy przejście od aktualnego stanu do nowego stanu odpowiadającego uzyskanemu wyrażeniu regularnemu.\n",
    "3. **Określenie stanów akceptujących:**\n",
    "   - Stany są akceptujące, jeśli ich odpowiadające wyrażenie regularne jest puste (nullable).\n",
    "4. **Powtarzanie procesu:**\n",
    "   - Kontynuujemy przetwarzanie aż do momentu, gdy nie zostaną odkryte nowe stany.\n",
    "\n",
    "### Implementacja\n",
    "\n",
    "#### Inicjalizacja struktur danych\n",
    "\n",
    "```python\n",
    "    states = set()  # Zbiór stanów (q0, q1, ...)\n",
    "    state_to_regex = {}  # Mapa stanów do odpowiadających im wyrażeń regularnych\n",
    "    accept_states = set()  # Zbiór stanów akceptujących\n",
    "    transitions = {}  # Mapa przejść (stan, symbol) -> nowy stan\n",
    "    regex_to_state = {}  # Mapa wyrażeń regularnych do stanów\n",
    "    \n",
    "    state_counter = 0  # Licznik stanów\n",
    "```\n",
    "\n",
    "- `states`: przechowuje unikalne identyfikatory stanów.\n",
    "- `state_to_regex`: odwzorowuje stany na odpowiadające im wyrażenia regularne.\n",
    "- `accept_states`: zawiera stany akceptujące.\n",
    "- `transitions`: mapa przejść `(stan, symbol) -> nowy stan`.\n",
    "- `regex_to_state`: odwzorowuje tekstową reprezentację wyrażenia regularnego na stan.\n",
    "- `state_counter`: licznik unikalnych identyfikatorów stanów.\n",
    "\n",
    "#### Funkcja pomocnicza do sprawdzania obecności elementu w liście\n",
    "\n",
    "```python\n",
    "    def is_in(element, array):\n",
    "        for ael in array:\n",
    "            if element == ael:\n",
    "                return True\n",
    "        return False\n",
    "```\n",
    "\n",
    "- Ta funkcja iteruje po tablicy `array` i sprawdza, czy dany `element` znajduje się w niej.\n",
    "- Jest używana do sprawdzania, czy dany regex już został odwiedzony.\n",
    "\n",
    "#### Przetwarzanie stanów za pomocą BFS\n",
    "\n",
    "```python\n",
    "    state_queue = deque()\n",
    "    state_queue.append(regex)\n",
    "    \n",
    "    while len(state_queue) != 0:\n",
    "        current_state: RegEx = state_queue.pop()\n",
    "        states.add(state_counter)\n",
    "```\n",
    "\n",
    "- `state_queue`: kolejka stanów do przetworzenia.\n",
    "- W pętli `while`, pobieramy stan z kolejki i dodajemy go do zbioru `states`.\n",
    "\n",
    "#### Określanie stanów akceptujących\n",
    "\n",
    "```python\n",
    "        if current_state.nullable():\n",
    "            accept_states.add(state_counter)\n",
    "```\n",
    "\n",
    "- Sprawdzamy, czy bieżący stan (`current_state`) akceptuje ciąg pusty (`nullable`). Jeśli tak, oznacza to, że stan ten jest akceptujący i dodajemy go do `accept_states`.\n",
    "\n",
    "#### Tworzenie przejść\n",
    "\n",
    "```python\n",
    "        state_to_regex[state_counter] = str(current_state)\n",
    "        regex_to_state[str(current_state)] = state_counter\n",
    "        \n",
    "        for symbol in alphabet:\n",
    "            new_state = current_state.derivative(symbol)\n",
    "            \n",
    "            if not is_in(str(new_state), list(map(lambda x: x[0], regex_to_state.items()))):\n",
    "                state_queue.append(new_state)\n",
    "            \n",
    "            transitions[(state_counter, symbol)] = new_state\n",
    "```\n",
    "\n",
    "- `current_state.derivative(symbol)`: obliczamy pochodną wyrażenia regularnego względem kolejnego symbolu z alfabetu.\n",
    "- Jeśli nowe wyrażenie regularne nie jest jeszcze w `regex_to_state`, dodajemy je do kolejki.\n",
    "- Rejestrujemy przejścia w `transitions`.\n",
    "\n",
    "\n",
    "#### Tworzenie i zwracanie DFA\n",
    "\n",
    "```python\n",
    "    dfa = DFA(states, alphabet, transitions, regex, accept_states)\n",
    "    return dfa\n",
    "```\n",
    "\n",
    "- Tworzymy obiekt DFA, przekazując do niego:\n",
    "  - `states`: zbiór stanów,\n",
    "  - `alphabet`: zbiór symboli alfabetu,\n",
    "  - `transitions`: mapa przejść między stanami,\n",
    "  - `regex`: jak założono w zadaniu - stan początkowy,\n",
    "  - `accept_states`: zbiór stanów akceptujących.\n",
    "- Zwracamy gotowy DFA.\n",
    "\n",
    "Kod został przetestowany i przeszedł wszystkie testy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
